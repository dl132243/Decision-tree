# Decision-tree

通常包括三个步骤： 特征选择， 决策树的生成和决策树的修剪

常见有ID3   C4.5  CRAT 三种方法

优点： 1.计算复杂度不高，输出结果易于理解
      2.对中间值的缺失不敏感
      3.可以处理不相关特征数据
      
缺点： 可能产生过度匹配的问题

ID3 选择信息增益为度量方式
C4.5 选择信息增益比为度量方式
CART 选择基尼指数为度量方式

重点学习了ID3算法： 以信息增益为度量方式

伪代码： 
 
      1. 读取分析数据

      2. 计算划分前的香农熵
      
      3. 划分数据集，计算划分数据集后的香农熵，选取最佳划分特征量
      
      4.利用递归构建决策树
      
      5.
