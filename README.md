# Decision-tree

通常包括三个步骤： 特征选择， 决策树的生成和决策树的修剪

常见有ID3   C4.5  CRAT 三种方法

优点： 1.计算复杂度不高，输出结果易于理解
      2.对中间值的缺失不敏感
      3.可以处理不相关特征数据
      
缺点： 可能产生过度匹配的问题

ID3 选择信息增益为度量方式
C4.5 选择信息增益比为度量方式
CART 选择基尼指数为度量方式

重点学习了ID3算法： 以信息增益为度量方式

伪代码： 
 
      1. 读取分析数据

      2. 计算划分前的香农熵
      
      3. 划分数据集，计算划分数据集后的香农熵，选取最佳划分特征量
      
      4.利用递归构建决策树
        
        递归结束条件是： 1. 所有特征属性都已遍历 或者 每个分支下的样本都属于同一类
        
                        2. 所有特征都已遍历后，而样本类别却不相同时，将其类别设为含样品较多的类别
      
      5.测试算法
      
      6. 决策树剪枝
         预先剪枝： 在决策树生成过程中，设置一定的限制条件，使其过拟合之前停止生长。比如设置一个阈值，当信息增益小于这个值时，决策树就停止生长，问题                    在于确定这个阈值也需要一定的依据，否则，阈值过大，容易造成欠拟合，阈值太小，容易过拟合。
         
         后剪枝：在决策树生成之后，自下而上的方式修建决策树
         
 构建决策树时，过多地考虑了如何提高训练数据的正确分类，从而构建了比较复杂的决策树，容易产生过拟合现象。
 
