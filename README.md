# Decision-tree

通常包括三个步骤： 特征选择， 决策树的生成和决策树的修剪

常见有ID3   C4.5  CRAT 三种方法

优点： 1.计算复杂度不高，输出结果易于理解
      2.对中间值的缺失不敏感
      3.可以处理不相关特征数据
      
缺点： 可能产生过度匹配的问题

ID3 选择信息增益为度量方式
C4.5 选择信息增益比为度量方式
CART 选择基尼指数为度量方式

重点学习了ID3算法： 以信息增益为度量方式

伪代码： 
 
      1. 读取分析数据

      2. 计算划分前的香农熵
      
      3. 划分数据集，计算划分数据集后的香农熵，选取最佳划分特征量
      
      4.利用递归构建决策树
        
        递归结束条件是： 1. 所有特征属性都已遍历 或者 每个分支下的样本都属于同一类
        
                        2. 所有特征都已遍历后，而样本类别却不相同时，将其类别设为含样品较多的类别
      
      5.测试算法
      
      6. 决策树剪枝
         预先剪枝： 在决策树生成过程中，设置一定的限制条件，使其过拟合之前停止生长。比如设置一个阈值，当信息增益小于这个值时，决策树就停止生长，问题                    在于确定这个阈值也需要一定的依据，否则，阈值过大，容易造成欠拟合，阈值太小，容易过拟合。
         
         后剪枝：在决策树生成之后，自下而上的方式修建决策树
         
 构建决策树时，过多地考虑了如何提高训练数据的正确分类，从而构建了比较复杂的决策树，容易产生过拟合现象。
 
 三大算法优缺点：
 
    1.ID3算法:使用’信息增益‘来进行构建,每次迭代选择信息增益最大的特征属性作为分割属性。只支持离散的特征属性
        优点:决策树构建速度快，实现简单
        缺点：算法依赖样本中出现次数较多的特征属性,但是出现次数最多的属性并不一定最优
    2.C4.5算法：使用’信息增益率‘来构建,在树的构建过程中会进行剪枝操作的优化,能够自动完成对连续属性的离散化处理。选择信息增益率大的属性进行分割
        优点:准确率较高,实现简单
        缺点：对数据集需要进行多次顺序扫描和排序,效率较低。
    3.CART算法:使用'基尼系数'作为数据纯度的量化指标来构建,选择‘GINI增益率’来分割，越大的即作为当前数据集的分割属性.可用于分类和回归。（二叉树构建）
        
    三种算法主要区别:CART构建的一定是二叉树,ID3,C4.5构建的不一定是二叉树

